\section{Background}
\label{sec:back}

This section provides some background on the need for some form of lockstepped execution in safety-related systems, and on the existing solutions to achieve it.


\subsection{Redundancy, Diversity and Sphere of Replication}
Safety-related systems are designed so that unreasonable risk due to software faults of any kind and systematic hardware faults is avoided by design, verification and validation (V\&V). However, random hardware faults cannot be avoided and appropriate safety measures need to be deployed, such as for instance, diverse redundancy for the highest safety integrity levels (SIL for short).

There are two main approaches to achieve diverse redundancy: using diverse hardware and/or software so that replicas (e.g. redundant threads) execution is diverse in nature, or using identical hardware and software and enforcing diversity by making identical replicas run on identical hardware (but not the same hardware unit) with some staggering so that hardware state is different at any point in time. Generally, the latter is preferred since it reduces design as well as V\&V costs because only one software unit and hardware unit needs to be designed, verified and validated, rather than having to do so for multiple units. This is, for instance, the case of DCLS in Infineon AURIX multicores~\cite{infineon_aurix}.

Lockstep operation is usually implemented 
comparing only off-core activities such as load and store requests, as well as interrupts and exceptions. Thus, cores execute software redundantly with some staggering and include some buffering capabilities to store off-core requests of the head thread until they are compared with the trail thread ones, as well as to store off-core responses for the trail thread, which are delivered to the corresponding core with some staggering w.r.t. the head thread.
Other off-core resources, such as storage elements (e.g. caches, memories) and communication elements (e.g. buses, crossbars) build upon ECC and CRC to reach diverse redundancy.


\subsection{Lockstep Strands}

\begin{figure}[t!]
\centering
%\setlength{\tabcolsep}{1.0mm}
\begin{tabular}{cc}
  \includegraphics[width=0.40\columnwidth]{imgs/HWlockstep.png} & 
  \includegraphics[width=0.50\columnwidth]{imgs/SWlockstep.png} \\
  (a) Hardware-only & (b) Software-only \\
\end{tabular}
  \caption{Schematic of hardware and software-based lockstep schemes.}
  \label{fig:HWSWlockstep}
\end{figure}

\textbf{Hardware-based tight lockstepping} builds upon two tightly coupled cores operating with identical state but with $N$ cycles of staggering, thus meaning that the head core is exactly $N$ cycles ahead in the exection of the trail core. Therefore, during fault-free operation, the trail core delivers exactly the same external signals as the head core but $N$ cycles later. This is managed with appropriate queues, as illustrated in Figure~\ref{fig:HWSWlockstep}(a). In particular, output activity of the head core needs to be stored during $N$ cycles until the trail core produces the same outputs. Then, they are compared and, if they match, the corresponding outputs are made visible out of the lockstepped cores complex, e.g. sending a load request to memory, storing data or signalling an interrupt. Externally, \emph{lockstepped cores are perceived as just one core} since only the activity generated by one of them is sent. Responses for those requests of a single core arrive just once, so they need to be replicated and delivered to both cores. In order to preserve the staggering, buffering is needed in the trail core side to keep data during $N$ cycles before delivering it to the core.
Such a scheme introduces a delay of $N$ cycles in any external access either for the outgoing requests (head core) or for the incoming responses (trail core). Nevertheless, $N$ is typically 2 or 3 cycles, so the impact of staggering is limited.

\textbf{Software-based light lockstepping} builds upon two cores able to operate independently executing different programs. Therefore, redundant threads need being created by software and scheduled accordingly into the head and trail cores which, in practice, are identical among them and inherit head or trail behavior only due to software management of their progress. The (simplified) operation, which we illustrate in Figure~\ref{fig:HWSWlockstep}(b), requires of a supporting monitor thread, which runs in another core, either in the same chip or another. For instance, this scheme has been devised to run the monitor in a processor with native hardware lockstepping, thus guaranteeing safety for the monitor, so that the monitor manages redundant threads of multiple software components running in a (likely powerful) multicore without lockstepping support. In particular, the monitor thread creates the two redundant processes and keeps the trail core stalled. Periodically, the monitor collects the number of instructions executed by the head and trail cores ($\#instr$ in the figure), and compares them. If the difference, $\#instr_{head} - \#instr_{trail}$, is above a given threshold $TH_{stag}$, then the trail core is allowed to make progress. Else, the trail core is stalled. Every $T_{check}$ cycles the monitor repeats the process. Note that $TH_{stag}$ needs to be carefully set to guarantee that, even if the head core got stalled and the trail one made progress at its maximum speed during $T_{check}$ cycles, the head core would still have a number of instructions executed higher than the trail one. If this is the case, both cores can be allowed to make progress unsupervisedly during $T_{check}$ cycles until the next monitoring check. 
There is an obvious relationship between the monitoring frequency and the staggering time: the looser the monitoring (i.e. the higher $T_{check}$) so that monitoring overhead decreases, the higher the staggering needed (i.e. the higher $TH_{stag}$). As shown in \cite{SergiDFT}, $TH_{stag}$ needs to be typically at least the maximum number of instructions that could be executed in 100$\mu$s.
Those 100$\mu$s are roughly the execution time increase that lockstepped execution will cause to let the trail core finish the execution of the trail thread after the head one does so with the head thread.


