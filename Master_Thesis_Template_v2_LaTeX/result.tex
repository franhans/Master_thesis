\clearpage\section{SafeDE Evaluation}

This section assesses SafeDE by different methods in the context of De-RISC MPSoC. To perform the evaluation process, we simulated the VHDL model of the MPSoC, including SafeDE IP. For that purpose, we used the simulator QuestaSim. We also synthesized the platform into a Xilinx Kintex UltraScale KCU105 evaluation kit and performed several tests. 

\subsection{Functional Validation}

This section assesses SafeDE by different methods in the context of De-RISC MPSoC. To perform the evaluation process, we simulated the VHDL model of the MPSoC, including SafeDE IP. For that purpose, we used the simulator QuestaSim. We also synthesized the platform into a Xilinx Kintex UltraScale KCU105 evaluation kit and performed several tests. 

\subsection{Functional Validation}

Several strategies are applied to validate the correct SafeDE functionality:

\textbf{Testbench:} The first approach to validating SafeDE is using a VHDL testbench that simulates the system's behavior by generating SafeDE inputs and reacting to its outputs. For that purpose, a component simulating the behavior of the cores is developed. This component generates the inst\_cnt signals for both cores, randomly setting its bits to '1' as if the cores were committing instructions. If the stall signal of one core is set to '1', this prevents the inst\_cnt signal bits of that core from being set to '1' as it would in a real core. 

In addition to this component,  procedures are defined at the testbench top to simulate APB read and write transactions. These procedures allow configuring the internal SafeDE registers during the simulation. As mentioned before, SafeDE has some statistic registers that can be read through the APB interface. During the testbench execution, the same statistics are computed at the top of the testbench so they can be compared with the results recovered from SafeDE. 

Therefore, the testbench execution starts configuring internal SafeDE registers. After that, the testbench runs for a configurable number of cycles simulating two cores executing instructions that hold when the stall signals rise. Finally, SafeDE is stopped and the statistics are retrieved. The testbench pass provided that the statistics read from SafeDE registers and the statistics computed at the top of the testbench coincide and the staggering keeps all the time between the limits (TH\_max\_stagg > stag > TH\_min\_stag)

The testbench completion is the first step for SafeDE design validation and it also eases the development process since the testbench is automatically run each time a new Git push is performed, informing of potential errors each time the design is modified.



\textbf{RISC-V ISA tests:} RISC-V ISA tests \cite{ISAtests} are a group of tests designed by the University of California to test the correct functioning of the RISC-V ISA instructions. The ISA tests are written in a single assembly language file and contain a self-checking code to test the result. Each ISA test tests one operation from the RISC-V ISA forcing corner cases. 

To load the binaries into the platform, control the execution flow and debug the application, we used GRMON. GRMON is a general debug monitor for the LEON (SPARC V7/V8) processor, NOEL-V (RISC-V) and for SOC designs based on the GRLIB IP library developed by CAES Gaisler. 
\textcolor{red}{add reference to GRMON} 

We modified the RISC-V ISA tests assembly code to configure and activate SafeDE prior to the test execution. We also performed some modifications to perform the execution in two different cores. The linker script is modified to load two ISA tests in two different address segments. GRMON is employed to activate both cores and determine the correct entry point. Once the test has finished, the results are stored in one register file register in each core. Those registers are read using GRMON to test that each test successfully passed. 

This process is automated via Makefiles (for the compilation), Bash scripts and GRMON TCL scripts. Therefore, a simple Linux command compiles the binaries, uploads the binaries to the FPGA, controls the execution flow and checks the results for all of the selected RISC-V ISA tests. The correct completion of these tests proves that neither the internal modifications performed to the integer pipeline of the cores nor the SafeDE action produces any system malfunctioning.

\textbf{TACLe Benchmarks:} TACLe Benchmarks suite \cite{falk2016taclebench} is a set of self-contained and open-source benchmarks of varying types and sizes. They are specifically designed for the evaluation of critical real-time embedded systems. Since they are self-contained, they have their inputs hardcoded together with the source code, making them perfect for experimenting in a bare metal setup. Their execution times range from thousand of cycles to a few millions of cycles, making them, in many cases, easy to simulate and debug to understand unexpected results.

We ported some TACLe Benchmarks to RISC-V ISA and added some C functions from the bare metal drive to configure SafeDE and gather execution statistics. As with the RISC-V ISA tests, TACLe Benchmarks also have a self-checking function that compares the obtained results (or a signature summarizing the final result) and the expected results. TACLe Benchmarks are executed over the FPGA loaded with the De-RISC platform bitstream. Execution is controlled by GRMON. The compilation, loading and result checking processes are automated using different scripts. 

We checked for all the executed TACLe Benchmarks that the results with SafeDE forcing 20 instructions of staggering (TH\_stag\_min = 20; TH\_stag\_max = 32750) coincide with the expected results. Also, results from the statistic registers are gathered for every benchmark. We checked that in every execution, the number of executed instructions by both cores coincide and that the minimum staggering reached during the execution is never below that of the expected one (stag >= TH\_min\_stag = 20). 

Since a 1-cycle latency exists between the moment the stall signal is asserted until the core pipeline stops making progress, the former staggering condition does not hold all the time during the execution of the benchmarks. Since cores are dual-issue, they can commit two instructions each cycle, being the worst scenario the one in which, with a previous 21 instructions staggering, the trail core commits two instructions while the head core commits none. When this happens, the staggering reaches 19 instructions, and trail core stall signal rises. Since that signal will take effect the next cycle, the trail core can commit another two instructions while the head cores commit none again, leaving the staggering at 17 instructions. The same happens for TH\_max\_stagg. The real minimum staggering that is kept is dependent on the architecture, namely on the number of issues of the core and the number of cycles that elapses from the moment the stall signal is asserted until it stalls the pipeline. 

\bigskip



\subsection{Fault Injection}
\textcolor{red}{Put something similar to what is written in the journal}
\bigskip



\subsection{Time Overhead}

To determine how much SafeDE afects the performance we have run the TACLe Benchmarks in three different scenarios:

1) Isolation: Only one core executes the benchamrk while the other remains idle.
2) Redundancy without diversity: Both cores execute the same benchmark without SafeDE enforcing any staggering. 
3) Diverse Redundancy: Both cores run the same benchmark and SafeDE is active enforcing a staggering of 20 instructions (TH\_stag\_min = 20)

As stated before, is convinient that the staggering is big enough to ensure that pipelines of both cores do not contain the same instruction in any of the pipeline stages at any point of the execution. Taking into account that NOEL-V cores are dual-issue with a 7 stages pipeline (pipelines can execute in parallel 14 instructions), 20 instructions of staggering (17 in practice) will suffice to avoid the two pipelines executing the same instruction. 


%processes are generated loading binaries twice (one for each
%core) in different memory segments. We execute each bench-
%mark 1,000 times and use the average cycle count for each one
%for our evaluation to discount the effect of small variations
%due to, for instance, delays caused by DRAM refreshes. In
%any case, absolute variations observed are always in the order
%of few tens of cycles.

%To discount the effect of effects such as DRAM refreshes
%and other minor performance variations, we run each bench-
%mark 1,000 times and report average cycle counts. In any case,
%variations observed are up to few tens of cycles across runs.
%As shown in Figure 6, performance degrades only by 0.3%
%on average (up to 1.3% for BITONIC) w.r.t. isolation runs,
%and 0.003%, so ≈0% (up to 0.6% for IIR) w.r.t. redundancy
%without diversity. Performance variations across runs, and even
%marginal performance gains with SafeDE relate to the initial
%state of the branch predictor, and to memory alignment of the
%binaries impacting the instruction cache behavior, since in the
%case of SafeDE we execute additional instructions to configure
%SafeDE. The relative effect of those variations can be larger
%for short programs, as it is the case for FAC, with variations
%in the range of 1%. In fact, those variations have a higher
%impact than the tiny performance degradation of SafeDE w.r.t.
%redundancy without diversity.
%Overall, performance overheads are tiny due the very low
%staggering threshold needed by SafeDE (20 cycles in our case),
%which is far lower than that for the software-only solution (e.g.
%100μs or more) [3].



%Results are shown in Figure 4. As shown, in all the cases,
%the execution time overhead with SafeDE w.r.t. the execution
%time in isolation and in two cores without SafeDE is negligible.
%In particular, SafeDE causes an execution time degradation in
%most of the cases below 0.5%, and up to 1.3% in one case
%(BITONIC benchmark) w.r.t. the execution time in isolation.
%If we compare it against the redundant execution without
%enforcing diversity, execution time degradation is generally
%below 0.1% (in some cases performance even marginally
%improves), and up to 0.6% for IIR benchmark.
%In some cases, minor performance variations between the
%isolation and redundant versions of the programs are observed,
%being those differences neither caused by interference between
%redundant threads, nor by SafeDE operation itself. Instead,
%those variations are caused by the initial core state (e.g. branch
%predictor state), or changes in instruction cache behavior due
%to changes in the memory alignment of the binaries with and
%without thread redundancy. In the case of FAC benchmark,
%since it is a small benchmark (around 700 instructions only),
%these tiny effects have a visible impact in relative terms
%(e.g. 1% execution time increase without SafeDE and 0.1%
%decrease with SafeDE).
%Overall, as expected, execution time increase can be re-
%garded as negligible since the staggering threshold can be
%set very low (20 cycles in our evaluation), thus far below
%the 100μs, which correspond to many thousands of cycles,
%imposed by the software-only solution [3].
\bigskip

\subsection{Hardware Costs}

We have employed the Vivado 2018.1 toolchain to synthesize and generate the bitstream targeting the Xilinx UlstraScale KCU105 Evaluation Kit featuring the Kintex XCKU040-2FFVA1156E FPGA. SafeDE implementation required 261 LUTs and 417 registers. Those numbers are really low compared with the resources required by each core (approximately 38,000 LUTs and 17,000 registers) or with the hardware resources required by the whole MPSoC (approximately 114,000 LUTs and 74,000 registers). Thus, SafeDE hardware costs are negligible. Namely, just 0.23\% of the LUTs and 0.56\% of the registers of the whole MPSoC are employed to implement SafeDE. Hardware overhead could be limited even more by removing the statistics registers which are only useful for debugging porpuses.

\textcolor{red}{add image of the pie maybe?}
\bigskip


